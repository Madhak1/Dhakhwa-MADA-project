---
title: "Cleaning data 'Superstore'"
author: "Malika Dhakhwa"
date: "2024-02-23"
output: html_document
---
# Setup

Install and load needed packages. 

```{r}
#| message: false
#| warning: false
library(readxl) #for loading Excel files
library(dplyr) #for data processing/cleaning
library(tidyr) #for data processing/cleaning
library(skimr) #for nice visualization of data 
library(here) #to set paths
library(ggplot2) # for visualization
library(scales) # for labels
```


# Data loading
Loading data using here function.


```{r}
# path to data
# note the use of the here() package and not absolute paths
data_location <- here::here("data","raw-data","superstoredata.csv")
raw_data <- read.csv(data_location)
```


# Check data

Checking data

```{r}
dplyr::glimpse(raw_data)
skimr::skim(raw_data)

```
# Cleaning

The purpose of this project is to study purchase pattern of customers residing outside of UK.The data set data_without_uk excludes all sales in the UK. It has 46431 observations with 9 variables. It is important to note that this data set accounts sales of a particular item as an observation. This means different items sold in the same Invoice appears as separate observations and it is most likely that an Invoice number could appear in separate observations. 

```{r}
# Exclude "United Kingdom" from the data set
data_without_uk <- subset(raw_data, Country != "United Kingdom")
skimr::skim(data_without_uk)
```

The data inspection showed missing values in CustomerID, issues with signs and distribution of Quantity and Sales and format of date. It also pointed out that InvoiceNo, StockCode, Description and Country are coded as character variables in the data which need to be changed to factors for better data manipulation.

First, there are 1480 missing values in CustomerID. Customer behavior is a crucial part of this analysis which cannot be tracked without the unique customerID. The observations without CustomerID do not help in doing meaningful analysis and are dropped. Further, CustomerID is coded as a numerical variable in the original data and it needs to be converted to factor variable. Country, Description and StockCode are also converted to factors from character variables.

```{r}
# Subset the data by excluding rows where CustomerID is NA
  clean_data_step1 <- data_without_uk %>%
  filter(!is.na(CustomerID)) %>%   #excludes observation without CustomerID
  mutate(InvoiceNo = as.factor(InvoiceNo),  #converts InvoiceNo to a factor
         CustomerID = as.factor(CustomerID), #converts CustomerID to a factor
         Country = as.factor(Country), #converts Country to a factor
         Description = as.factor(Description), #converts Description to a factor
         StockCode = as.factor(StockCode)) #converts StockCode to a factor

# Checking the cleaned data
skimr::skim(clean_data_step1)

```

There are negative values in quantity and sales due to order cancellation. An inspection is carried out to check the number of such observations are in the data set. It is found that there are 1372 observations with negative Quantity and Sales.

```{r}
#Filter observations where Quantity and Sales are negative
negative_sales <- clean_data_step1 %>%
  filter (Quantity < 0 & Sales < 0)
# Count the negative sales
count(negative_sales)
```

Next step is to find matching original sales entries corresponding to the negative entries based on CustomerID, StockCode, Quantity and Sales figures. 1372 matches were found, which is equal to the no. of observations with negative Quantity and Sales figures.  

```{r}
positive_sales <- clean_data_step1 %>%
  filter(Quantity > 0 & Sales > 0)
count(positive_sales)

#Checking for matches
matched_sales <- negative_sales %>%
  rowwise() %>%
  mutate(match_found = any(StockCode %in% positive_sales$StockCode &
                             CustomerID %in% positive_sales$CustomerID))

count(matched_sales)
```

#The next attempt is to match returns to their original sales and get rid of both matched transactions. However, this needs further work as the below codes are resulting 582 observations in original sales instead of 1372.

```{r}
# Add an identifier to each row in both data sets to track them (Marked as comment because this chunk of code needs further work and also takes a longer time to run)

"negative_sales <- negative_sales %>% mutate(neg_id = row_number())
positive_sales <- positive_sales %>% mutate(pos_id = row_number())

# Initialize a vector to keep track of matched negative_sales' IDs
matched_neg_ids <- integer(0)

# Initialize an empty data frame to store matches
matches <- data.frame(neg_id = integer(), pos_id = integer())

# Iterate over positive_sales to find matches in  (Marked as comment because it takes a longer time to run this code)
for (pos_row in 1:nrow(positive_sales)) {
  pos_sale <- positive_sales[pos_row, ]
  
  # Attempt to match with negative_sales, excluding already matched rows
  potential_matches <- negative_sales %>%
    filter(
      !neg_id %in% matched_neg_ids, # Exclude already matched ones
      CustomerID == pos_sale$CustomerID,
      StockCode == pos_sale$StockCode,
      Quantity == -pos_sale$Quantity, # Matching negative of Quantity
      Sales == -pos_sale$Sales # Matching negative of Sales
    )
  
  if (nrow(potential_matches) > 0) {
    # If a match is found, take the first one
    first_match <- potential_matches[1, ]
    matched_neg_ids <- c(matched_neg_ids, first_match$neg_id)
    
    # Record the match
    matches <- rbind(matches, data.frame(neg_id = first_match$neg_id, pos_id = pos_sale$pos_id))
  }
}

# Extract matched rows based on IDs 
matched_negative_sales <- negative_sales %>% filter(neg_id %in% matches$neg_id)
matched_positive_sales <- positive_sales %>% filter(pos_id %in% matches$pos_id)
print(matched_negative_sales)
print(matched_positive_sales)"
```

Observations with negative Quantity and Sales are dropped though unsuccessful in finding exact no. of matching original transactions. This brought down the total number of observations to 43563.  

```{r}
clean_data_step2 <- clean_data_step1 %>%
  filter(Quantity > 0, Sales > 0)
skimr::skim(clean_data_step2)
```

Inspection is required in the distribution of Quantity, unit price and Sales, the higher values of these variable are concentrated at the higher percentiles. I focused on distribution of UnitPrice at small intervals from 75% to 100%.

```{r}
# Calculate higher percentiles within the 3rd quartile to max range
percentiles <- quantile(clean_data_step2$UnitPrice, probs = c(0.75, 0.90, 0.95, 0.99, 0.995, 0.996, 0.997, 0.998, 0.999, 1))
print(percentiles)
```

The results indicate that there are only limited items that are above the unit price of 25. Observations wherein UnitPrice is above 25 are filtered for further examination. It resulted into 433 observations.

```{r}
# Filter rows where UnitPrice is greater than 25

UnitPrice_above25 <- clean_data_step2[clean_data_step2$UnitPrice > 25, ]

str(UnitPrice_above25)
```

The components of description was checked using the unique function which resulted into 2903 levels for 433 observations. With the help of ChatGPT, it was found that in R, when a subset of a data frame is created, the factor levels in the subset are not automatically dropped even if they are not present in the subset. To resolve this, the unused levels required to be dropped using the droplevels() function.

```{r}
#Attempt without dropping the unused factor levels
unique_categories_initial <- unique(UnitPrice_above25$Description)
print(unique_categories_initial)

# Dropping unused factor levels in the Description column
UnitPrice_above25$Description <- droplevels(UnitPrice_above25$Description)

# Finding the unique categories again
unique_categories_final <- unique(UnitPrice_above25$Description)
print(unique_categories_final)
```

The above code resulted into 19 categories in product description which have the Unitprice above 25. Some of the high UnitPrice are related to the descriptions such as CARRIAGE, POSTAGE, Manual, appearing to be related to logistics. 1216 observations with such descriptions were dropped from the data since those are less likely to be directly related to customer purchase. The no. of observations came down to 42347.

```{r}
# Remove observations with specified descriptions
clean_data_step3 <- clean_data_step2[!grepl("Manual", clean_data_step2$Description) &
                                !grepl("POSTAGE", clean_data_step2$Description) &
                                !grepl("CARRIAGE", clean_data_step2$Description), ]

# Inspect the cleaned data
skimr::skim(clean_data_step3)

```
Next the InovoiceDate is in character format, and needs to be converted to date format.

```{r}
clean_data_step3$InvoiceDate <- as.POSIXct(clean_data_step3$InvoiceDate, format = "%m/%d/%Y %H:%M")

skimr::skim(clean_data_step3)

```

The store also sells merchandises in wholesale quantities. As this study focuses on end user purchases, observations of sales to wholesales need to be disregarded. A wholesaler potentially purchases in big volumes and in higher frequencies. In absence of clear information to identify wholesalers, a scatter plot of Total Invoice Value by Purchase Frequency could provide helpful information to identify potential wholesalers. 

```{r}
# Calculate purchase frequency and total invoice value per customer
customer_stats <- clean_data_step3 %>%
  group_by(CustomerID) %>%  
  summarise(PurchaseFrequency = n_distinct(InvoiceNo),
            TotalInvoiceValue = sum(Sales, na.rm = TRUE)) %>%
  ungroup()  # Data ungrouped for further analysis

# Scatter plot of Total Invoice Value by Purchase Frequency
p1 <- ggplot(customer_stats, aes(x = PurchaseFrequency, y = TotalInvoiceValue)) +
  geom_point(alpha = 0.5) +  # Alpha can be adjusted as needed for point transparency
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Adding a linear regression line without standard error
  labs(title = "Total Invoice Value by Purchase Frequency",
       x = "Purchase Frequency",
       y = "Total Invoice Value") +
  scale_y_continuous(labels = label_comma()) +  # This line changes the y-axis labels from exp. number to actual value
  
    theme_minimal()
plot(p1)


figure_file = here("results","figures","InvoiceValue_by_frequency.png")
ggsave(filename = figure_file, plot=p1)

skimr::skim(customer_stats)

```

The scatter plot reveals that most of the customers made purchases less than 50 times during that period and most of the cumulative purchase values were less than 50,000. Assuming that the outliers represent purchases by wholesalers, observations with purchase frequencies above 50 and cumulative Invoice value above 50,000 are dropped from the data.

This involves first sub-setting customer_stats such that it includes only those observations where PurchaseFrequency <= 50 and TotalInvoiceValue <=50000 and finally, merging this sub-set with the original dataset (clean_data_step3).

```{r}

# Filtering the customer_stats to meet the conditions
filtered_customer_stats <- customer_stats %>%
  filter(PurchaseFrequency <= 50, TotalInvoiceValue <= 50000)

# Excluding purchase frequencies above 50 and cumulative invoice values above 50000 from the data
clean_data_step4 <- clean_data_step3 %>%
  semi_join(filtered_customer_stats, by='CustomerID')

skimr::skim(clean_data_step4)
```

This step excluded 9745 observations from the data for a final total observation of 32602. A similar scatter plot is plotted for the new data.

```{r}
# Calculate purchase frequency and total invoice value per customer
customer_stats4 <- clean_data_step4 %>%
  group_by(CustomerID) %>%
  summarise(PurchaseFrequency = n_distinct(InvoiceNo),
            TotalInvoiceValue = sum(Sales, na.rm = TRUE)) %>%
  ungroup()  # Ensure the data is ungrouped for further analysis

# Scatter plot of Total Invoice Value by Purchase Frequency
p2 <- ggplot(customer_stats4, aes(x = PurchaseFrequency, y = TotalInvoiceValue)) +
  geom_point(alpha = 0.5) +  # Adjust alpha for point transparency, if needed
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add a linear regression line without standard error
  labs(title = "Total Invoice Value by Purchase Frequency",
       x = "Purchase Frequency",
       y = "Total Invoice Value") +
  scale_y_continuous(labels = label_comma()) +  # This line changes the y-axis labels
  
    theme_minimal()
plot(p2)

```

The distribution of the data appears relatively homogeneous. 

The wholesalers are expected to purchase in large quantity. Additional inspection is carried out to identify wholesalers by quantity purchased and individual Purchase Amount by plotting a scatter plot. 

```{r}
# Calculate purchase frequency and total invoice value per customer
customer_stats5 <- clean_data_step4 %>%
  group_by(InvoiceNo) %>%
  summarise(InvoiceQty = sum(Quantity, na.rm = TRUE),
            InvoiceValue = sum(Sales, na.rm = TRUE)) %>%
  ungroup()  # Ensure the data is ungrouped for further analysis

# Scatter plot of Total Invoice Value by Purchase Frequency
p3 <- ggplot(customer_stats5, aes(x = InvoiceValue, y = InvoiceQty)) +
  geom_point(alpha = 0.5) +  # Adjust alpha for point transparency, if needed
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add a linear regression line without standard error
  labs(title = "Total Invoice Value by Purchase Frequency",
       x = "Invoice Amount",
       y = "Quantity Purchased") +
  scale_y_continuous(labels = label_comma()) +  # This line changes the y-axis labels
  
    theme_minimal()
plot(p3)

```
The scatter plot reveals that most of the customers made purchases less than 1000 items and most of the invoice values were less than 2,500. Assuming that the sparsely distributed points represent purchases by wholesalers, observations with Quantity above 1000 and individual Invoice value above 2,500 are removed from the data.

This involves first sub-setting customer_stats5 such that it includes only those observations where InvoiceQty <= 1000 and InvoiceValue <=2500 and finally, merging this sub-set with the original dataset (clean_data_step4).

```{r}
# Filtering the customer_stats5 to meet the conditions
filtered_customer_stats5 <- customer_stats5 %>%
  filter(InvoiceQty <= 1000, InvoiceValue <= 2500)

# Excluding purchase quantities above 1000 and individual invoice values above 2500 from the data
clean_data_step5 <- clean_data_step4 %>%
  semi_join(filtered_customer_stats5, by='InvoiceNo')

skimr::skim(clean_data_step5)
```
This step excluded 4485 observations from the data for a final total observation of 28117. A similar scatter plot is plotted for the new data.

```{r}
# Calculate purchase quantity and amount per invoice
customer_stats6 <- clean_data_step5 %>%
  group_by(InvoiceNo) %>%
  summarise(InvoiceQty = sum(Quantity, na.rm = TRUE),
            InvoiceValue = sum(Sales, na.rm = TRUE)) %>%
  ungroup()  # Ensure the data is ungrouped for further analysis

# Scatter plot of Purchase Quantity by Invoice Value
p4 <- ggplot(customer_stats6, aes(x = InvoiceValue, y = InvoiceQty)) +
  geom_point(alpha = 0.5) +  # Adjust alpha for point transparency, if needed
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add a linear regression line without standard error
  labs(title = "Purchase Quantity by Invoice Value",
       x = "Invoice Amount",
       y = "Quantity Purchased") +
  scale_y_continuous(labels = label_comma()) +  # This line changes the y-axis labels
  
    theme_minimal()
plot(p4)

```


```{r}

```


Assigning a final name to the subset to make it more relevant to the actual data.

```{r}
processed_superstore <- clean_data_step5

str(processed_superstore)
```
It is important to note that this data accounts sales of a particular item at a time as an observation leading different items sold in the same Invoice to appear as separate observations. As such most of the Invoice numbers appear several times in different observations of the data. A meaningful analysis of customer spending pattern can be done by total Invoice Sales. So another data is created where each Invoice is a unique observation and can be used for groupoing customers based on Recency, Frequency and Monetary value. 

```{r}
processed_superstore_RFM <- processed_superstore %>%
  group_by(InvoiceNo) %>%
  summarise(InvoiceDate = first(InvoiceDate),
            CustomerID = first(CustomerID),
            InvoiceValue = sum(Sales, na.rm = TRUE),
            Country = first(Country)) %>%
select(InvoiceDate, InvoiceNo, CustomerID, InvoiceValue, Country)


str(processed_superstore_RFM)
```

# Save data 

Finally, the cleaned data is saved as RDS file. As learnt in the class, saving the cleaned data as RDS or RDA/Rdata files, preserves coding like factors, characters, numeric, etc.  Saving as CSV, that information would get lost. However, CSV is better for sharing with others since it's plain text. 

See here for some suggestions on how to store your processed data:
http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata

```{r}
save_data_location <- here::here("data","processed-data","processed_superstore.rds")
saveRDS(processed_superstore, file = save_data_location)

```

```{r}
save_data_location <- here::here("data","processed-data","processed_superstore_RFM.rds")
saveRDS(processed_superstore_RFM, file = save_data_location)

```


