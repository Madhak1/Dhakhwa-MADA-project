---
title: "An example cleaning script"
author: "Andreas Handel"
date: "2023-01-03"
output: html_document
---


# Processing script

This contains the same code and comments/information as `processingcode.R`.

This just shows it as Quarto file, to give you an idea how to do it in a setup that combines code and text in a single file.

See the other Quarto file for my currently preferred approach of pulling code from the R script into the Quarto file.


# Setup

Load needed packages. make sure they are installed.

```{r}
library(readxl) #for loading Excel files
library(dplyr) #for data processing/cleaning
library(tidyr) #for data processing/cleaning
library(skimr) #for nice visualization of data 
library(here) #to set paths
```


# Data loading

Note that for functions that come from specific packages (instead of base R), I often specify both package and function like so:
package::function() that's not required one could just call the function specifying the package makes it clearer where the function "lives",
but it adds typing. You can do it either way.

```{r}
# path to data
# note the use of the here() package and not absolute paths
data_location <- here::here("data","raw-data","superstoredata.csv")
raw_data <- read.csv(data_location)
```


# Check data

Several ways of looking at the data

```{r}
dplyr::glimpse(raw_data)
summary(raw_data)
head(raw_data)
skimr::skim(raw_data)
```



# Cleaning

The data inspection showed missing values in CustomerID, issues with signs and distribution of Quantity and Sales and format of date. It also pointed out that InvoiceNo, StockCode, Description and Country are coded as character variables in the data which need to be changed to factors for better data manipulation.

First, there are 135080 missing values in CustomerID. Customer behavior is a crucial part of this analysis which cannot be tracked without the unique customerID The observations without CustomerID do not help in doing meaningful analysis and hence dropped. Further, CustomerID is coded as a numerical variable in the data and it needs to be converted to factor variable. Country, Description and StockCode are also converted to factors from character variables.

```{r}
# Subset the data by excluding rows where CustomerID is NA
  clean_data_step1 <- raw_data %>%
  filter(!is.na(CustomerID)) %>%
  mutate(InvoiceNo = as.factor(InvoiceNo),
         CustomerID = as.factor(CustomerID),
         Country = as.factor(Country),
         Description = as.factor(Description),
         StockCode = as.factor(StockCode))

# Check the dimensions of the cleaned dataset
dim(clean_data_step1)
```

There are negative values in quantity and sales due to order cancellation. An inspection is required to check how many of such observations are in the data set. It is found that there are 8905 observations with negative Quantity and Sales.

```{r}
#Filter observations where Quantity and Sales are negative
negative_sales <- clean_data_step1 %>%
  filter (Quantity < 0 & Sales < 0)
# Count the negative sales
count(negative_sales)
```

Next step is to find matching original sales entries corresponding to the negative entries based on CustomerID, StockCode, Quantity and Sales figures. 8905 matches were found, which is equal to the no. of observations with negative Quantity and Sales figures.  

```{r}
positive_sales <- clean_data_step1 %>%
  filter(Quantity > 0 & Sales > 0)
count(positive_sales)

#Checking for matches
matched_sales <- negative_sales %>%
  rowwise() %>%
  mutate(match_found = any(StockCode %in% positive_sales$StockCode &
                             CustomerID %in% positive_sales$CustomerID))

count(matched_sales)
```

The next attempt is to match returns to their original sales and get rid of both matched transactions. However, this needs further work as the below codes are resulting 3072 observations in original sales instead of 8905.

```{r}
# Add an identifier to each row in both data sets to track them (Marked as comment because this chunk of code needs further work and also takes a longer time to run)

"negative_sales <- negative_sales %>% mutate(neg_id = row_number())
positive_sales <- positive_sales %>% mutate(pos_id = row_number())

# Initialize a vector to keep track of matched negative_sales' IDs
matched_neg_ids <- integer(0)

# Initialize an empty data frame to store matches
matches <- data.frame(neg_id = integer(), pos_id = integer())"

# Iterate over positive_sales to find matches in  (Marked as comment because it takes a longer time to run this code)
"for (pos_row in 1:nrow(positive_sales)) {
  pos_sale <- positive_sales[pos_row, ]
  
  # Attempt to match with negative_sales, excluding already matched rows
  potential_matches <- negative_sales %>%
    filter(
      !neg_id %in% matched_neg_ids, # Exclude already matched
      CustomerID == pos_sale$CustomerID,
      StockCode == pos_sale$StockCode,
      Quantity == -pos_sale$Quantity, # Matching negative of Quantity
      Sales == -pos_sale$Sales # Matching negative of Sales
    )
  
  if (nrow(potential_matches) > 0) {
    # If a match is found, take the first one
    first_match <- potential_matches[1, ]
    matched_neg_ids <- c(matched_neg_ids, first_match$neg_id)
    
    # Record the match
    matches <- rbind(matches, data.frame(neg_id = first_match$neg_id, pos_id = pos_sale$pos_id))
  }
}

# Extract matched rows based on IDs 
#matched_negative_sales <- negative_sales %>% filter(neg_id %in% matches$neg_id)
#matched_positive_sales <- positive_sales %>% filter(pos_id %in% matches$pos_id)"
```

Observations with negative Quantity and Sales are dropped though unsuccessful in finding exact no. of matching original transactions.  


```{r}
clean_data_step2 <- clean_data_step1 %>%
  filter(Quantity > 0, Sales > 0)
```

Inspection is required in the distribution of Quantity, unit price and Sales, the higher values of these variable are concentrated at the 100 percentile. I focused on distribution of UnitPrice at small intervals from 75% to 100%.

```{r}
# Calculate higher percentiles within the 3rd quartile to max range
percentiles <- quantile(clean_data_step2$UnitPrice, probs = c(0.75, 0.90, 0.95, 0.99, 0.995, 0.996, 0.997, 0.998, 0.999, 1))
print(percentiles)
```

The results indicate that there are only limited items that are above the unit price of 41.89. Observations wherein UnitPrice is above 50 are filtered. It resulted into 241 observations with 14 levels of Description.

```{r}
# Filter rows where UnitPrice is greater than 50

UnitPrice_above50 <- clean_data_step2[clean_data_step2$UnitPrice > 50, ]

#check the no. of categoriesis in description
n_distinct(UnitPrice_above50$Description)
```

The components of description was checked using the unique function which resulted into 3896 levels for 241 observations. With the help of ChatGPT, it was found that in R, when a subset of a data frame is created, the factor levels in the subset are not automatically dropped even if they are not present in the subset. To resolve this, the unused levels required to be dropped using the droplevels() function.

```{r}
#Attempt without dropping the unused factor levels
unique(UnitPrice_above50$Description)

# Dropping unused factor levels in the Description column
UnitPrice_above50$Description <- droplevels(UnitPrice_above50$Description)

# Finding the unique categories again
unique_categories <- unique(UnitPrice_above50$Description)

# Print the unique categories
print(unique_categories)

```

The above code resulted into 14 categories in product description. Most of the high UnitPrice are related to the descriptions such as Manual, POSTAGE, DOTCOM POSTAGE, appearing to be related to logistics. I got rid of observations with all those levels in description since those are less likely to be directly related to customer purchase.

```{r}
# Remove observations with specified descriptions
clean_data_step3 <- clean_data_step2[!grepl("Manual", clean_data_step2$Description) &
                                !grepl("POSTAGE", clean_data_step2$Description) &
                                !grepl("DOTCOM POSTAGE", clean_data_step2$Description), ]

# Inspect the cleaned data
skimr::skim(clean_data_step3)

```

Next the InovoiceDate is in character format, and needs to be converted to date format.

```{r}
clean_data_step3$InvoiceDate <- as.POSIXct(clean_data_step3$InvoiceDate, format = "%m/%d/%Y %H:%M")

skimr::skim(clean_data_step3)

```


The data 'clean_data_step3 is clean now. The purpose of this project is to study purchase pattern of customers residing outside of UK.The final data set data_without_uk excludes all sales in the UK. It has 42455 observations. 

```{r}
# Exclude "United Kingdom" from the data set
data_without_uk <- subset(clean_data_step3, Country != "United Kingdom")

skimr::skim(data_without_uk)
```

A spike in Quantity and Sales observed between 75 and 100 percentile. The store also sells merchandises in wholesale quantities. As this study focuses on end user purchases, observations appearing to sales to wholesales need to be dropped. This calls for a close view of distribution of Sales by Invoice value. 

```{r}

# Sum sales by InvoiceNo to get total invoice value
invoice_totals <- data_without_uk %>%
  group_by(InvoiceNo) %>%
  summarise(TotalInvoiceValue = sum(Sales, na.rm = TRUE))

# Calculate percentiles of total invoice value
percentiles_InvoiceValue <- quantile(invoice_totals$TotalInvoiceValue, 
                                     probs = c(0.25, 0.50, 0.75, 0.80, 0.85, 0.86, 0.87, 0.88, 0.89, 0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.99, 1), 
                                     na.rm = TRUE)


print(percentiles_InvoiceValue)
```
Checking the distribution of quantity.
```{r}
library(dplyr)

# Sum quantities by InvoiceNo to get total invoice quantity
invoice_quantity_totals <- data_without_uk %>%
  group_by(InvoiceNo) %>%
  summarise(TotalInvoiceQuantity = sum(Quantity, na.rm = TRUE))

# Calculate percentiles of total invoice quantity
percentiles_InvoiceQuantity <- quantile(invoice_quantity_totals$TotalInvoiceQuantity, 
                                        probs = c(0.25, 0.50, 0.75, 0.80, 0.85, 0.86, 0.87, 0.88, 0.89, 0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.99, 1), 
                                        na.rm = TRUE)

print(percentiles_InvoiceQuantity)
```
Invoice Value and Quantity have different distribution pattern. Based on the products the store sales, it is difficult to make a decision based on the Invoice Quantity. So, the sample is filtered to 90 percentile of Invoice Value subject to further investigation.


```{r}
# Calculate the 90th percentile of Invoice value
quantity_90th_percentile <- quantile(data_without_uk$Quantity, 0.9)

# Create a new data frame excluding observations with Quantity above the 90th percentile
data_without_uk1 <- data_without_uk %>%
  filter(Quantity <= quantity_90th_percentile)

```



Assigning a final name to the subset to make it easier to add further cleaning steps.

```{r}
processed_superstore <- data_without_uk1
```


# Save data 

Finally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. 
This preserves coding like factors, characters, numeric, etc.  If you save as CSV, that information would get lost.
However, CSV is better for sharing with others since it's plain text. If you do CSV, you might want to write down somewhere what each variable is.

See here for some suggestions on how to store your processed data:
http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata

```{r}
save_data_location <- here::here("data","processed-data","processed_superstore.rds")
saveRDS(processed_superstore, file = save_data_location)


```



# Notes

Removing anyone who had "faulty" or missing data is one approach. It's often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep individuals with some missing information).

